<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Large-Scale, Collaborative Effort to Estimate the Reproducibility of Psychological Science Open Science Collaboration</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability resp="authors"  status="unknown">
					<!-- the @rest attribute above gives the document copyrights owner (publisher, authors), if known -->
					<licence/>
				</availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<note type="raw_affiliation">University of Virginia , Department of Psychology , 102 Gilmer Hall , Box 400400 , Charlottesville , VA 22904</note>
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">University of Virginia</orgName>
								<address>
									<addrLine>102 Gilmer Hall</addrLine>
									<postBox>Box 400400</postBox>
									<postCode>22904</postCode>
									<settlement>Charlottesville</settlement>
									<region>VA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Large-Scale, Collaborative Effort to Estimate the Reproducibility of Psychological Science Open Science Collaboration</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3E641E4A7B77B9BB6C2888FB7379A8FC</idno>
					<idno type="DOI">10.1177/1745691612462588</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-09-09T08:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=1, consolidateHeader=1, consolidateFunders=0, includeRawAffiliations=true, includeRawCitations=true, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>methodology</term>
					<term>replication</term>
					<term>reproducibility</term>
					<term>psychological science</term>
					<term>open</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Reproducibility is a defining feature of science. However, because of strong incentives for innovation and weak incentives for confirmation, direct replication is rarely practiced or published. The Reproducibility Project is an open, large-scale, collaborative effort to systematically examine the rate and predictors of reproducibility in psychological science. So far, 72 volunteer researchers from 41 institutions have organized to openly and transparently replicate studies published in three prominent psychological journals in 2008. Multiple methods will be used to evaluate the findings, calculate an empirical rate of replication, and investigate factors that predict reproducibility. Whatever the result, a better understanding of reproducibility will ultimately improve confidence in scientific methodology and findings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Reproducibility-the extent to which consistent results are observed when scientific studies are repeated-is one of science's defining features <ref type="bibr" target="#b0">(Bacon, 1267</ref><ref type="bibr" target="#b0">(Bacon, /1859;;</ref><ref type="bibr" target="#b8">Jasny, Chin, Chong, &amp; Vignieri, 2011;</ref><ref type="bibr" target="#b10">Kuhn, 1962;</ref><ref type="bibr" target="#b13">Popper, 1934</ref><ref type="bibr" target="#b13">Popper, /1992;;</ref><ref type="bibr" target="#b16">Rosenthal, 1991)</ref>, 2 and has even been described as the "demarcation criterion between science and nonscience" <ref type="bibr">(Braude, 1979, p. 2)</ref>. In principle, the entire body of scientific evidence could be reproduced independently by researchers following the original methods and drawing from insights gleaned by prior investigators. In this sense, belief in scientific evidence is not contingent on trust in its originators. Other types of belief depend on the authority and motivations of the source; beliefs in science do not. 3  Considering its central importance, one might expect replication to be a prominent part of scientific practice. It is not <ref type="bibr" target="#b4">(Collins, 1985;</ref><ref type="bibr" target="#b15">Reid, Soley, &amp; Wimmer, 1981;</ref><ref type="bibr" target="#b17">Schmidt, 2009)</ref>. An important reason for this is that scientists have strong incentives to introduce new ideas but weak incentives to confirm the validity of old ideas <ref type="bibr" target="#b12">(Nosek, Spies, &amp; Motyl, 2012)</ref>. Innovative findings produce rewards of publication, employment, and tenure; replicated findings produce a shrug.</p><p>Devoting resources to confirmation instead of innovation is a poor investment if the original findings are valid. But the costs of accepting false findings are high as well. Burgeoning research areas could fruitlessly expend resources in the pursuit of false leads, and theories could rely on invalid empirical evidence. A wise apportionment of resources between innovation and confirmation would take into account the reproducibility rate to maximize the rate of knowledge accumulation. How would resources be allocated if the reproducibility rate were 90%? What about 30%?</p><p>There exists very little evidence to provide reproducibility estimates for scientific fields, though some empirically informed estimates are disquieting <ref type="bibr" target="#b7">(Ioannidis, 2005)</ref>. When independent researchers tried to replicate dozens of important studies on cancer, women's health, and cardiovascular disease, only 25% of their replication studies confirmed the original result <ref type="bibr" target="#b14">(Prinz, Schlange, &amp; Asadullah, 2011)</ref>. In a similar investigation, <ref type="bibr" target="#b1">Begley and Ellis (2012)</ref> reported a meager 11% replication rate. In psychology, a survey of unpublished replication attempts found that about 50% replicated the original results <ref type="bibr" target="#b6">(Hartshorne &amp; Schachner, 2012</ref>; see also <ref type="bibr">Wager, Lindquist, Nichols, Kober, &amp; van Snellenberg, 2009, on reproducibility in neuroscience)</ref>. In this paper, we introduce the Reproducibility Project: an effort to systematically estimate the reproducibility rate of psychological science as it is practiced currently, and to investigate factors that predict reproducibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Reproducibility Project</head><p>Obtaining a meaningful estimate of reproducibility requires conducting replications of a sizable number of studies. However, because of existing incentive structures, it is not in an individual scientist's professional interest to conduct numerous replications. The Reproducibility Project addresses these barriers by spreading the workload over a large number of researchers. As of August 23, 2012, 72 volunteers from 41 institutions had joined the replication effort. Each contributor plays an important but circumscribed role, such as by contributing on a team conducting one replication study. Researchers volunteer to contribute on the basis of their interests, skills, and available resources. Information about the project's coordination, planning, materials, and execution is available publicly on the Open Science Framework's Web site (<ref type="url" target="http://openscienceframework">http://openscienceframework</ref>. org/). Open practices increase the accountability of the replication team and, ideally, the quality of the designs and results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Selecting Studies for Replication</head><p>Studies eligible for replication were selected from 2008 issues of three prominent journals that differ in topical emphasis and publishing format (i.e., short reports vs. long-form articles): Journal of Experimental Psychology: Learning, Memory, and Cognition, Journal of Personality and Social Psychology, and Psychological Science. 4 To minimize selection biases even within this restricted sample, replication teams choose from among the first 30 articles published in an issue. From the selected article, each team selects a key finding from a single study for replication (the last study by default, unless it is unfeasible to replicate). As eligible articles are claimed, additional articles from the sampling frame are made available for selection. Not all studies can be replicated. For example, some used unique samples or specialized equipment that is unavailable, and others were dependent on a specific historical event.</p><p>Although feasibility constraints can reduce the generalizability of the ultimate results, they are inevitably part and parcel of reproducibility itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conducting the Replications</head><p>The project's replication attempts follow a standardized protocol aimed at minimizing irrelevant variation in data collection and reporting methods, and maximizing the quality of replication efforts. The project attempts direct replications-"repetition of an experimental procedure" in order to "verify a piece of knowledge" <ref type="bibr">(Schmidt, 2009, p. 92, 93)</ref>. Replications must have high statistical power (1-β ≥ .80 for the effect size of the original study) and use the original materials, if they are available. Researchers solicit feedback on their research design from the original authors before collecting data, particularly to identify factors that may interfere with replication. Identified threats are either remedied with revisions or coded as potential predictors of reproducibility and written into the replication report.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation of Replication-Study Results</head><p>Successful replication can be defined by "vote-counting," either narrowly (i.e., obtaining the same statistically significant effect as original study) or broadly (i.e., obtaining a directionally similar, but not necessarily statistically significant, result), or quantitatively defined-for example, through meta-analytic estimates combining the original and replication study, comparisons of effect sizes, or updated estimates of Bayesian priors. As yet, there is no single general, standard answer to the question "What is replication?" so we employ multiple criteria <ref type="bibr" target="#b19">(Valentine et al., 2011)</ref>.</p><p>Failures to replicate might result from several factors. The first is a simple Type II error with an occurrence rate of 1-β: Some true findings will fail to replicate purely by chance. However, the overall replication rate can be measured against the average statistical power across studies. For this reason, the project focuses on the overall reproducibility rate. Individual studies that fail to replicate are not treated as disconfirmed. Failures to replicate can also occur if (a) the original effect is false; (b) the actual size of the effect is lower than originally reported, making it more difficult to detect; (c) the design, implementation, or analysis of either the original or replication study is flawed; or (d) the replication methodology differs from the original methodology in ways that are critical for successful replication. 5 All of these reasons are important to consider in evaluations of reproducibility, but the most interesting may be the last. Identifying specific ways in which replications and original studies differ, especially when replications fail, can advance the theoretical understanding of previously unconsidered conditions necessary to obtain an effect. Thus, replication is theoretically consequential.</p><p>The most important point is that a failure to replicate an effect does not conclusively indicate that the original effect was false. An effect may also fail to replicate because of insufficient power, problems with the design of the replication study, or limiting conditions, whether known or unknown. For this reason, the Reproducibility Project investigates factors such as replication power, the evaluation of the replicationstudy design by the original authors, and the original study's sample and effect sizes as predictors of reproducibility. Identifying the contribution of these factors to reproducibility is useful because each has distinct implications for interventions to improve reproducibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implications of the Reproducibility Project</head><p>An estimate of the reproducibility of current psychological science will be an important first. A high reproducibility estimate might boost confidence in conventional research and peerreview practices in the face of criticisms about inappropriate flexibility in design, analysis, and reporting that can inflate the rate of false positives <ref type="bibr" target="#b5">(Greenwald, 1975;</ref><ref type="bibr" target="#b9">John, Loewenstein, &amp; Prelec, 2012;</ref><ref type="bibr" target="#b18">Simmons, Nelson, &amp; Simonsohn, 2011)</ref>. A low estimate might prompt reflection on the quality of standard practice, motivate further investigation of reproducibility, and ultimately lead to changes in practice and publishing standards <ref type="bibr" target="#b2">(Bertamini &amp; Munafò, 2012;</ref><ref type="bibr" target="#b11">LeBel &amp; Peters, 2011)</ref>. Some may worry that the discovery of a low reproducibility rate will damage the image of psychology or of science more generally. It is certainly possible that opponents of science will use such a result to renew their calls to reduce funding for basic research. However, we believe that the alternative is much worse: having a low reproducibility rate, but failing to investigate and discover it. If reproducibility is lower than acceptable, then it is vitally important that we know about it in order to address it. Self-critique, and the promise of self-correction, is what makes science such an important part of humanity's effort to understand nature and ourselves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>The Reproducibility Project uses an open methodology to test the reproducibility of psychological science. It also models procedures designed to simplify and improve reproducibility. Readers can review the discussion history of the project, examine the project's design and structured protocol, retrieve replication materials from the various teams, obtain reports or raw data from completed replications, and join the project to conduct a replication (start here: <ref type="url" target="http://openscienceframework">http://openscienceframework</ref> .org/project/EZcUj/). Increasing the community of volunteers will strengthen the power and impact of the project. With this open, large-scale, collaborative scientific effort, we hope to identify the factors that contribute to the reproducibility and validity of psychological science. Ultimately, such evidenceand steps toward resolution, if the evidence produces a call for action-can improve psychological science's most important asset: confidence in its methodology and findings.</p></div>		</body>
		<back>

			<div type="acknowledgement">
<div><p>Notes 1. <rs type="person">Anita Alexander</rs>, <rs type="affiliation">University of Virginia</rs>; <rs type="person">Michael Barnett-Cowan</rs>, The <rs type="affiliation">Brain and Mind Institute, University of Western Ontario</rs>; <rs type="person">Elizabeth Bartmess</rs>, <rs type="affiliation">University of California, San Francisco</rs>; <rs type="person">Frank A. Bosco</rs>, <rs type="affiliation">Marshall University</rs>; <rs type="person">Mark Brandt</rs>, <rs type="affiliation">Tilburg University</rs>; <rs type="person">Joshua Carp</rs>, <rs type="affiliation">University of Michigan</rs>; <rs type="person">Jesse J. Chandler</rs>, <rs type="affiliation">Princeton University</rs>; <rs type="person">Russ Clay</rs>, <rs type="affiliation">University of Richmond</rs>; <rs type="person">Hayley Cleary</rs>, <rs type="affiliation">Virginia Commonwealth University</rs>; <rs type="person">Michael Cohn</rs>, <rs type="affiliation">University of California, San Francisco</rs>; <rs type="person">Giulio Costantini</rs>, <rs type="affiliation">University of Milano-Bicocca</rs>; <rs type="person">Jamie DeCoster</rs>, <rs type="affiliation">University of Virginia</rs>; <rs type="person">Elizabeth Dunn</rs>, <rs type="affiliation">University of British Columbia</rs>; <rs type="person">Casey Eggleston</rs>, <rs type="affiliation">University of Virginia</rs>; <rs type="person">Vivien Estel</rs>, <rs type="affiliation">University of Erfurt</rs>; <rs type="person">Frank J. Farach</rs>, <rs type="affiliation">University of Washington</rs>; <rs type="person">Jenelle Feather</rs>, <rs type="affiliation">Massachusetts Institute of Technology</rs>; <rs type="person">Susann Fiedler</rs>, <rs type="affiliation">Max Planck Institute for Research on Collective Goods</rs>; <rs type="person">James G. Field</rs>, <rs type="affiliation">Marshall University</rs>; <rs type="person">Joshua D. Foster</rs>, <rs type="affiliation">University of South Alabama</rs>; <rs type="person">Michael Frank</rs>, <rs type="affiliation">Stanford University</rs>; <rs type="person">Rebecca S. Frazier</rs>, <rs type="affiliation">University of Virginia</rs>; <rs type="person">Heather M. Fuchs</rs>, <rs type="affiliation">University of Cologne</rs>; <rs type="person">Jeff Galak</rs>, <rs type="affiliation">Carnegie Mellon University</rs>; <rs type="person">Elisa Maria Galliani</rs>, <rs type="affiliation">University of Padova</rs>; <rs type="person">Sara García</rs>, <rs type="affiliation">Universidad Nacional de Asunción</rs>; <rs type="person">Elise M. Giammanco</rs>, <rs type="affiliation">University of Virginia</rs>; <rs type="person">Elizabeth A. Gilbert</rs>, <rs type="affiliation">University of Virginia</rs>; <rs type="person">Roger Giner-Sorolla</rs>, <rs type="affiliation">University of Kent</rs>; <rs type="person">Lars Goellner</rs>, <rs type="affiliation">University of Erfurt</rs>; <rs type="person">Jin X. Goh</rs>, <rs type="affiliation">Northeastern University</rs>; <rs type="person">R. Justin Goss</rs>, <rs type="affiliation">University of Texas at San Antonio</rs>; <rs type="person">Jesse Graham</rs>, <rs type="affiliation">University of Southern California</rs>; <rs type="person">James A. Grange</rs>, <rs type="affiliation">Keele University</rs>; <rs type="person">Jeremy R. Gray</rs>, <rs type="affiliation">Michigan State University</rs>; <rs type="person">Sarah Gripshover</rs>, <rs type="affiliation">Stanford University</rs>; <rs type="person">Joshua Hartshorne</rs>, <rs type="affiliation">Massachusetts Institute of Technology</rs>; <rs type="person">Timothy B. Hayes</rs>, <rs type="affiliation">University of Southern California</rs>; <rs type="person">Georg Jahn</rs>, <rs type="affiliation">University of Greifswald</rs>; <rs type="person">Kate Johnson</rs>, <rs type="affiliation">University of Southern California</rs>; <rs type="person">William Johnston</rs>, <rs type="affiliation">Massachusetts Institute of Technology</rs>; <rs type="person">Jennifer A. Joy-Gaba</rs>, <rs type="affiliation">Virginia Commonwealth University</rs>; <rs type="person">Calvin K. Lai</rs>, <rs type="affiliation">University of Virginia</rs>; <rs type="person">Daniel Lakens</rs>, <rs type="affiliation">Eindhoven University of Technology</rs>; <rs type="person">Kristin Lane</rs>, <rs type="affiliation">Bard College</rs>; <rs type="person">Etienne P. LeBel</rs>, <rs type="affiliation">University of Western Ontario</rs>; <rs type="person">Minha Lee</rs>, <rs type="affiliation">University of Virginia</rs>; <rs type="person">Kristi Lemm</rs>, <rs type="affiliation">Western Washington University</rs>; <rs type="person">Sean Mackinnon</rs>, <rs type="affiliation">Dalhousie University</rs>; <rs type="person">Michael May</rs>, <rs type="affiliation">University of Bonn</rs>; <rs type="person">Katherine Moore</rs>, <rs type="affiliation">Elmhurst College</rs>; <rs type="person">Matt Motyl</rs>, <rs type="affiliation">University of Virginia</rs>; <rs type="person">Stephanie M. Müller</rs>, <rs type="affiliation">University of Erfurt</rs>; <rs type="person">Marcus Munafo</rs>, <rs type="affiliation">University of Bristol</rs>; <rs type="person">Brian A. Nosek</rs>, <rs type="affiliation">University of Virginia</rs>; <rs type="person">Catherine Olsson</rs>, <rs type="affiliation">Massachusetts Institute of Technology</rs>; <rs type="person">Dave Paunesku</rs>, <rs type="affiliation">Stanford University</rs>; <rs type="person">Marco Perugini</rs>, <rs type="affiliation">University of Milano-Bicocca</rs>; <rs type="person">Michael Pitts</rs>, <rs type="affiliation">Reed College</rs>; <rs type="person">Kate Ratliff</rs>, <rs type="affiliation">University of Florida</rs>; <rs type="person">Frank Renkewitz</rs>, <rs type="affiliation">University of Erfurt</rs>; <rs type="person">Abraham M. Rutchick</rs>, <rs type="affiliation">California State University, Northridge</rs>; <rs type="person">Gillian Sandstrom</rs>, <rs type="affiliation">University of British Columbia</rs>; <rs type="person">Rebecca Saxe</rs>, <rs type="affiliation">Massachusetts Institute of Technology; Dylan Selterman, University of Maryland</rs>; <rs type="person">William Simpson</rs>, <rs type="affiliation">University of Virginia</rs>; <rs type="person">Colin Tucker Smith</rs>, <rs type="affiliation">University of Florida</rs>; <rs type="person">Jeffrey R. Spies</rs>, <rs type="affiliation">University of Virginia</rs>; <rs type="person">Nina Strohminger</rs>, <rs type="affiliation">Duke University</rs>; <rs type="person">Thomas Talhelm</rs>, <rs type="affiliation">University of Virginia</rs>; <rs type="person">Anna van</rs> 't Veer, <rs type="affiliation">Tilburg University</rs>; <rs type="person">Michelangelo Vianello</rs>, <rs type="affiliation">University of Padova</rs>.</p><p>2. Some distinguish between "reproducibility" and "replicability" by treating the former as a narrower case of the latter (e.g., computational sciences) or vice versa (e.g., biological sciences). We ignore the distinction. 3. That is, they are not supposed to matter. To the extent that they do is evidence of current scientific practices relying on authority rather than evidence. 4. Additional journals may be added in the future if enough volunteers join the project. 5. Note that the Reproducibility Project does not evaluate whether the original interpretation of the finding is correct. For example, if an eligible study had an apparent confound in its design, that confound would be retained in the replication attempt. Confirmation of theoretical interpretations is an independent consideration.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Declaration of Conflicting Interests</head><p>The authors declared that they had no conflicts of interest with respect to their authorship or the publication of this article.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<author>
			<persName><forename type="first">R</forename><surname>Bacon</surname></persName>
		</author>
		<ptr target="http://books.google.com/books?id=wMUKAAAAYAAJ" />
	</analytic>
	<monogr>
		<title level="m">Fr. Rogeri Bacon Opera quaedam hactenus inedita</title>
		<title level="s">. containing I.-Opus tertium</title>
		<imprint>
			<publisher>Longman, Green, Longman and Roberts</publisher>
			<date type="published" when="1267">1859. 1267</date>
			<biblScope unit="volume">I</biblScope>
		</imprint>
	</monogr>
	<note>II.-Opus minus III.-Compendium philosophiae</note>
	<note type="raw_reference">Bacon, R. (1859). Fr. Rogeri Bacon Opera quaedam hactenus inedita. Vol. I. containing I.-Opus tertium. II.-Opus minus. III.-Compendium philosophiae. Longman, Green, Longman and Roberts. Retrieved from http://books.google.com/books?id= wMUKAAAAYAAJ (Original work published 1267).</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Raise standards for preclinical cancer research</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Begley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Ellis</surname></persName>
		</author>
		<idno type="DOI">10.1038/483531a</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">483</biblScope>
			<biblScope unit="page" from="531" to="533" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Begley, C. G., &amp; Ellis, L. M. (2012). Raise standards for preclinical cancer research. Nature, 483, 531-533. doi:10.1038/483531a</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bite-size science and its undesired side effects</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bertamini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Munafò</surname></persName>
		</author>
		<idno type="DOI">10.1177/1745691611429353</idno>
	</analytic>
	<monogr>
		<title level="j">Perspectives on Psychological Science</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="67" to="71" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Bertamini, M., &amp; Munafò, M. R. (2012). Bite-size science and its undesired side effects. Perspectives on Psychological Science, 7, 67-71. doi:10.1177/1745691611429353</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Braude</surname></persName>
		</author>
		<title level="m">ESP and psychokinesis. A philosophical examination</title>
		<meeting><address><addrLine>Philadelphia, PA</addrLine></address></meeting>
		<imprint>
			<publisher>Temple University Press</publisher>
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Braude, S. E. (1979). ESP and psychokinesis. A philosophical exami- nation. Philadelphia, PA: Temple University Press.</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Changing order</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Collins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985">1985</date>
			<publisher>Sage</publisher>
			<pubPlace>London, England</pubPlace>
		</imprint>
	</monogr>
	<note type="raw_reference">Collins, H. M. (1985). Changing order. London, England: Sage.</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Consequences of prejudice against the null hypothesis</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Greenwald</surname></persName>
		</author>
		<idno type="DOI">10.1037/h0076157</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Greenwald, A. G. (1975). Consequences of prejudice against the null hypothesis. Psychological Bulletin, 82, 1-20. doi:10.1037/ h0076157</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Tracking replicability as a method of post-publication open evaluation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Hartshorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schachner</surname></persName>
		</author>
		<idno type="DOI">10.3389/fncom.2012.00008</idno>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Computational Neuroscience</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Hartshorne, J. K., &amp; Schachner, A. (2012). Tracking replicability as a method of post-publication open evaluation. Frontiers in Com- putational Neuroscience, 6, 8. doi: 10.3389/fncom.2012.00008</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Why most published research findings are false</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P A</forename><surname>Ioannidis</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pmed.0020124</idno>
		<ptr target="http://www.plosmedicine.org/article/info:doi/10.1371/journal.pmed.0020124" />
	</analytic>
	<monogr>
		<title level="j">PLoS Medicine</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">124</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ioannidis, J. P. A. (2005). Why most published research findings are false. PLoS Medicine, 2(8), e124. Retrieved from http://www .plosmedicine.org/article/info:doi/10.1371/journal.pmed.0020124</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Again, and again, and again</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Jasny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vignieri</surname></persName>
		</author>
		<idno type="DOI">10.1126/sci-ence.334.6060.1225</idno>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">334</biblScope>
			<biblScope unit="page">1225</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Jasny, B. R., Chin, G., Chong, L., &amp; Vignieri, S. (2011). Again, and again, and again . . . Science, 334, 1225. doi:10.1126/sci- ence.334.6060.1225</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Measuring the prevalence of questionable research practices with incentives for truth telling</title>
		<author>
			<persName><forename type="first">L</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Loewenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Prelec</surname></persName>
		</author>
		<idno type="DOI">10.1177/0956797611430953</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="524" to="532" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="raw_reference">John, L., Loewenstein, G., &amp; Prelec, D. (2012). Measuring the prevalence of questionable research practices with incentives for truth telling. Psychological Science, 23, 524-532. doi:10.1177/ 0956797611430953</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Kuhn</surname></persName>
		</author>
		<title level="m">The structure of scientific revolutions</title>
		<meeting><address><addrLine>Chicago, IL</addrLine></address></meeting>
		<imprint>
			<publisher>University of Chicago Press</publisher>
			<date type="published" when="1962">1962</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Kuhn, T. S. (1962). The structure of scientific revolutions. Chicago, IL: University of Chicago Press.</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fearing the future of empirical psychology: Bem&apos;s (2011) evidence of psi as a case study of deficiencies in modal research practice</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Lebel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Peters</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0025172</idno>
	</analytic>
	<monogr>
		<title level="j">Review of General Psychology</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="371" to="379" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="raw_reference">LeBel, E. P., &amp; Peters, K. R. (2011). Fearing the future of empirical psychology: Bem&apos;s (2011) evidence of psi as a case study of defi- ciencies in modal research practice. Review of General Psychol- ogy, 15, 371-379. doi:10.1037/a0025172</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Scientific utopia: II. Restructuring incentives and practices to promote truth over publishability</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Nosek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Spies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Motyl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perspectives on Psychological Science</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="615" to="631" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Nosek, B. A., Spies, J. R., &amp; Motyl, M. (2012). Scientific utopia: II. Restructuring incentives and practices to promote truth over pub- lishability. Perspectives on Psychological Science, 7, 615-631.</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The logic of scientific discovery</title>
		<author>
			<persName><forename type="first">K</forename><surname>Popper</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1934">1992. 1934</date>
			<publisher>Routledge</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
	<note>Original work</note>
	<note type="raw_reference">Popper, K. (1992). The logic of scientific discovery. New York, NY: Routledge. (Original work published 1934)</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Believe it or not: How much can we rely on published data on potential drug targets?</title>
		<author>
			<persName><forename type="first">F</forename><surname>Prinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schlange</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Asadullah</surname></persName>
		</author>
		<idno type="DOI">10.1038/nrd3439-c1</idno>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Drug Discovery</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="712" to="713" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Prinz, F., Schlange, T., &amp; Asadullah, K. (2011). Believe it or not: How much can we rely on published data on potential drug tar- gets? Nature Reviews Drug Discovery, 10, 712-713. doi:10.1038/ nrd3439-c1</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Replication in advertising research: 1977</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">N</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Soley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Wimmer</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0149-2063_03_00024-2</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Advertising</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="3" to="13" />
			<date type="published" when="1978">1981. 1978. 1979</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Reid, L. N., Soley, L. C., &amp; Wimmer, R. D. (1981). Replication in advertising research: 1977, 1978, 1979. Journal of Advertising, 10, 3-13. doi:10.1016/S0149-2063_03_00024-2</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Replication in behavioral research</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rosenthal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Replication research in the social sciences</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Neuliep</surname></persName>
		</editor>
		<meeting><address><addrLine>Newbury Park, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Sage</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="1" to="39" />
		</imprint>
	</monogr>
	<note type="raw_reference">Rosenthal, R. (1991). Replication in behavioral research. In J. W. Neuliep (Ed.), Replication research in the social sciences (pp. 1-39). Newbury Park, CA: Sage.</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Shall we really do it again? The powerful concept of replication is neglected in the social sciences</title>
		<author>
			<persName><forename type="first">S</forename><surname>Schmidt</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0015108</idno>
	</analytic>
	<monogr>
		<title level="j">Review of General Psychology</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="90" to="100" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Schmidt, S. (2009). Shall we really do it again? The powerful concept of replication is neglected in the social sciences. Review of Gen- eral Psychology, 13, 90-100. doi:10.1037/a0015108</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Falsepositive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Simmons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Simonsohn</surname></persName>
		</author>
		<idno type="DOI">10.1177/0956797611417632</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1359" to="1366" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Simmons, J. P., Nelson, L. D., &amp; Simonsohn, U. (2011). False- positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant. Psy- chological Science, 22, 1359-1366. doi:10.1177/09567976114 17632</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Replication in prevention science</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Valentine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Biglan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Boruch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">G</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Flay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">.</forename><surname>Schinke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename></persName>
		</author>
		<idno type="DOI">10.1007/s11121-011-0217-6</idno>
	</analytic>
	<monogr>
		<title level="j">Prevention Science</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="103" to="117" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Valentine, J. C., Biglan, A., Boruch, R. F., Castro, F. G., Collins, L. M., Flay, B. R., . . . Schinke, S. P. (2011). Replication in pre- vention science. Prevention Science, 12, 103-117. doi:10.1007/ s11121-011-0217-6</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Evaluating the consistency and specificity of neuroimaging data using meta-analysis</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Wager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Lindquist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Nichols</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kober</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">X</forename><surname>Van Snellenberg</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuroimage.2008.10.061</idno>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="210" to="S221" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Wager, T. D., Lindquist, M. A., Nichols, T. E., Kober, H., &amp; van Snel- lenberg, J. X. (2009). Evaluating the consistency and specific- ity of neuroimaging data using meta-analysis. NeuroImage, 45, S210-S221. doi:10.1016/j.neuroimage.2008.10.061</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
